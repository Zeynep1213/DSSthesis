{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7ed47f-21af-4c35-8a20-4d0a6e9561e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: top2vec in /opt/anaconda3/lib/python3.9/site-packages (1.0.27)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.9/site-packages (from top2vec) (1.3.4)\n",
      "Requirement already satisfied: wordcloud in /opt/anaconda3/lib/python3.9/site-packages (from top2vec) (1.8.1)\n",
      "Requirement already satisfied: umap-learn>=0.5.1 in /opt/anaconda3/lib/python3.9/site-packages (from top2vec) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/anaconda3/lib/python3.9/site-packages (from top2vec) (1.20.3)\n",
      "Requirement already satisfied: hdbscan>=0.8.27 in /opt/anaconda3/lib/python3.9/site-packages (from top2vec) (0.8.28)\n",
      "Requirement already satisfied: gensim>=4.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from top2vec) (4.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.9/site-packages (from gensim>=4.0.0->top2vec) (6.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/lib/python3.9/site-packages (from gensim>=4.0.0->top2vec) (1.7.1)\n",
      "Requirement already satisfied: cython>=0.27 in /opt/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.27->top2vec) (0.29.24)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /opt/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.27->top2vec) (0.24.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.27->top2vec) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.20->hdbscan>=0.8.27->top2vec) (2.2.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/anaconda3/lib/python3.9/site-packages (from umap-learn>=0.5.1->top2vec) (0.5.7)\n",
      "Requirement already satisfied: numba>=0.49 in /opt/anaconda3/lib/python3.9/site-packages (from umap-learn>=0.5.1->top2vec) (0.54.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from umap-learn>=0.5.1->top2vec) (4.62.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec) (58.0.4)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /opt/anaconda3/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec) (0.37.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.9/site-packages (from pandas->top2vec) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.9/site-packages (from pandas->top2vec) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->top2vec) (1.16.0)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.9/site-packages (from wordcloud->top2vec) (8.4.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.9/site-packages (from wordcloud->top2vec) (3.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud->top2vec) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud->top2vec) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud->top2vec) (3.0.4)\n",
      "Requirement already satisfied: top2vec[sentence_encoders] in /opt/anaconda3/lib/python3.9/site-packages (1.0.27)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.9/site-packages (from top2vec[sentence_encoders]) (1.3.4)\n",
      "Requirement already satisfied: hdbscan>=0.8.27 in /opt/anaconda3/lib/python3.9/site-packages (from top2vec[sentence_encoders]) (0.8.28)\n",
      "Requirement already satisfied: umap-learn>=0.5.1 in /opt/anaconda3/lib/python3.9/site-packages (from top2vec[sentence_encoders]) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/anaconda3/lib/python3.9/site-packages (from top2vec[sentence_encoders]) (1.20.3)\n",
      "Requirement already satisfied: gensim>=4.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from top2vec[sentence_encoders]) (4.2.0)\n",
      "Requirement already satisfied: wordcloud in /opt/anaconda3/lib/python3.9/site-packages (from top2vec[sentence_encoders]) (1.8.1)\n",
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.9/site-packages (from top2vec[sentence_encoders]) (2.9.0)\n",
      "Requirement already satisfied: tensorflow-hub in /opt/anaconda3/lib/python3.9/site-packages (from top2vec[sentence_encoders]) (0.12.0)\n",
      "Requirement already satisfied: tensorflow-text in /opt/anaconda3/lib/python3.9/site-packages (from top2vec[sentence_encoders]) (2.9.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.9/site-packages (from gensim>=4.0.0->top2vec[sentence_encoders]) (6.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/lib/python3.9/site-packages (from gensim>=4.0.0->top2vec[sentence_encoders]) (1.7.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (1.1.0)\n",
      "Requirement already satisfied: cython>=0.27 in /opt/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (0.29.24)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /opt/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.20->hdbscan>=0.8.27->top2vec[sentence_encoders]) (2.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (4.62.3)\n",
      "Requirement already satisfied: numba>=0.49 in /opt/anaconda3/lib/python3.9/site-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.54.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/anaconda3/lib/python3.9/site-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.5.7)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /opt/anaconda3/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[sentence_encoders]) (58.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.9/site-packages (from pandas->top2vec[sentence_encoders]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.9/site-packages (from pandas->top2vec[sentence_encoders]) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->top2vec[sentence_encoders]) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (0.26.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (2.9.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (2.9.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (3.10.0.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (0.2.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (21.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (2.9.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (1.12)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (1.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (14.0.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (0.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (1.46.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (3.20.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow->top2vec[sentence_encoders]) (1.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow->top2vec[sentence_encoders]) (0.37.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (2.26.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (2.0.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (2.6.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (5.1.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (3.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->top2vec[sentence_encoders]) (3.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow->top2vec[sentence_encoders]) (3.0.4)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.9/site-packages (from wordcloud->top2vec[sentence_encoders]) (8.4.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.9/site-packages (from wordcloud->top2vec[sentence_encoders]) (3.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "\n",
    "!pip install top2vec\n",
    "!pip install top2vec[sentence_encoders]\n",
    "from top2vec import Top2Vec\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f979611f-6da8-489d-909d-ca7547318ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: redditcleaner in /opt/anaconda3/lib/python3.9/site-packages (1.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install redditcleaner\n",
    "import re\n",
    "import redditcleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd7224e-52e4-4c5d-b337-12cd88d9510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(csv_file_name):\n",
    "    import re\n",
    "    import redditcleaner\n",
    "    df = pd.read_csv(csv_file_name, lineterminator=\"\\n\") ## reading the csv file\n",
    "    df = df[~df['selftext'].isin(['[removed]', '[deleted]' ])].dropna(subset=['selftext']) ## dropping removed and deleted posts\n",
    "    df['selftext'] = df['selftext'].map(redditcleaner.clean) ## cleaning text of reddit specific punctuations\n",
    "    df['selftext'] = df['selftext'].map(lambda x: re.sub(r\"[^A-Za-z ]\", '', x)) ## cleaning punctuations\n",
    "    df['selftext'] = df['selftext'].map(lambda x: re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', x)) ## cleaning url\n",
    "    df['selftext'] = df['selftext'].apply(lambda x: x.lower()) ## lowercase\n",
    "    df['selftext'] = df.selftext.values.tolist()\n",
    "#    df['date'] = pd.to_datetime(df['date']).dt.to_period('D') \n",
    "#    df['date'] = df['date'].dt.strftime('%d-%m-%Y') ##formatting date into d-m-y \n",
    "    df = df[['url', 'selftext']]\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029deda7-5f14-4367-8aad-9507a3ae36d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = clean_data(\"file_name.csv\")\n",
    "df_pre['post_wordcount'] = df_pre['selftext'].str.count(' ') + 1\n",
    "df_pre_clean = df_pre[df_pre.post_wordcount > 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0a8e208-b151-4f8c-823e-2fcf530f0952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>selftext</th>\n",
       "      <th>post_wordcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://19thnews.org/2021/07/the-covid-delta-v...</td>\n",
       "      <td>the highly contagious delta variant of covid  ...</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://19thnews.org/2020/11/karen-bass-addres...</td>\n",
       "      <td>were the only newsroom dedicated to writing ab...</td>\n",
       "      <td>913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://19thnews.org/2020/08/kamala-harris-com...</td>\n",
       "      <td>read the latest story on how lgbtq americans a...</td>\n",
       "      <td>1582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://19thnews.org/2020/11/kim-ng-mlb-first-...</td>\n",
       "      <td>were the only newsroom dedicated to writing ab...</td>\n",
       "      <td>868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://19thnews.org/2020/12/shirley-sherrod-h...</td>\n",
       "      <td>as a nonprofit newsroom members are critical t...</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2714</th>\n",
       "      <td>https://www.zerohedge.com/political/state-wash...</td>\n",
       "      <td>authored by jonathan turley the house democrat...</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>https://www.zerohedge.com/geopolitical/biden-k...</td>\n",
       "      <td>the biden administration has said it will cont...</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2716</th>\n",
       "      <td>https://www.zerohedge.com/political/we-want-re...</td>\n",
       "      <td>update et someone on twitter points out that a...</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>https://www.zerohedge.com/technology/here-are-...</td>\n",
       "      <td>over the past week president trump has been ki...</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2718</th>\n",
       "      <td>https://www.zerohedge.com/political/dc-descend...</td>\n",
       "      <td>remember what happened after the last inaugura...</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2692 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "0     https://19thnews.org/2021/07/the-covid-delta-v...   \n",
       "1     https://19thnews.org/2020/11/karen-bass-addres...   \n",
       "2     https://19thnews.org/2020/08/kamala-harris-com...   \n",
       "3     https://19thnews.org/2020/11/kim-ng-mlb-first-...   \n",
       "4     https://19thnews.org/2020/12/shirley-sherrod-h...   \n",
       "...                                                 ...   \n",
       "2714  https://www.zerohedge.com/political/state-wash...   \n",
       "2715  https://www.zerohedge.com/geopolitical/biden-k...   \n",
       "2716  https://www.zerohedge.com/political/we-want-re...   \n",
       "2717  https://www.zerohedge.com/technology/here-are-...   \n",
       "2718  https://www.zerohedge.com/political/dc-descend...   \n",
       "\n",
       "                                               selftext  post_wordcount  \n",
       "0     the highly contagious delta variant of covid  ...             775  \n",
       "1     were the only newsroom dedicated to writing ab...             913  \n",
       "2     read the latest story on how lgbtq americans a...            1582  \n",
       "3     were the only newsroom dedicated to writing ab...             868  \n",
       "4     as a nonprofit newsroom members are critical t...            1150  \n",
       "...                                                 ...             ...  \n",
       "2714  authored by jonathan turley the house democrat...             341  \n",
       "2715  the biden administration has said it will cont...             325  \n",
       "2716  update et someone on twitter points out that a...             810  \n",
       "2717  over the past week president trump has been ki...             825  \n",
       "2718  remember what happened after the last inaugura...             606  \n",
       "\n",
       "[2692 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frames = [df_pre_clean] \n",
    "df = pd.concat(frames)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b5a1f-0087-416b-8d3c-bb6ae1c59aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 14:40:37,575 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-05-18 14:40:46,032 - top2vec - INFO - Downloading universal-sentence-encoder model\n",
      "2022-05-18 14:41:45.797641: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-18 14:41:58,967 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2022-05-18 14:42:19,125 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2022-05-18 14:42:35,159 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2022-05-18 14:42:35,318 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2022-05-18 14:43:05,310 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Min Count: 5 & Number of Topics: 2 & Coherence:0.8464550645125578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 14:43:13,749 - top2vec - INFO - Downloading universal-sentence-encoder model\n",
      "INFO:top2vec:Downloading universal-sentence-encoder model\n",
      "2022-05-18 14:43:27,870 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2022-05-18 14:43:47,075 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2022-05-18 14:44:01,337 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2022-05-18 14:44:01,501 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2022-05-18 14:45:01,456 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Min Count: 10 & Number of Topics: 34 & Coherence:0.6514580155603966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 14:45:10,064 - top2vec - INFO - Downloading universal-sentence-encoder model\n",
      "INFO:top2vec:Downloading universal-sentence-encoder model\n",
      "2022-05-18 14:45:25,935 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2022-05-18 14:45:47,474 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2022-05-18 14:46:02,850 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2022-05-18 14:46:03,026 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2022-05-18 14:46:33,898 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Min Count: 15 & Number of Topics: 2 & Coherence:0.7538062869096442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 14:46:42,417 - top2vec - INFO - Downloading universal-sentence-encoder model\n",
      "INFO:top2vec:Downloading universal-sentence-encoder model\n",
      "2022-05-18 14:46:58,575 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2022-05-18 14:47:20,021 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2022-05-18 14:47:37,413 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2022-05-18 14:47:37,588 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2022-05-18 14:48:03,179 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Min Count: 20 & Number of Topics: 2 & Coherence:0.7320953433270296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 14:48:12,465 - top2vec - INFO - Downloading universal-sentence-encoder model\n",
      "INFO:top2vec:Downloading universal-sentence-encoder model\n",
      "2022-05-18 14:48:26,275 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2022-05-18 14:48:46,798 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2022-05-18 14:49:02,363 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2022-05-18 14:49:02,521 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2022-05-18 14:49:31,138 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Min Count: 25 & Number of Topics: 2 & Coherence:0.6931288998039375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 14:49:40,512 - top2vec - INFO - Downloading universal-sentence-encoder model\n",
      "INFO:top2vec:Downloading universal-sentence-encoder model\n",
      "2022-05-18 14:49:51,530 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2022-05-18 14:50:09,364 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2022-05-18 14:50:24,400 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2022-05-18 14:50:24,557 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2022-05-18 14:50:50,726 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Min Count: 30 & Number of Topics: 2 & Coherence:0.6434535771890731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 14:50:59,305 - top2vec - INFO - Downloading universal-sentence-encoder model\n",
      "INFO:top2vec:Downloading universal-sentence-encoder model\n",
      "2022-05-18 14:51:12,821 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2022-05-18 14:51:32,622 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n"
     ]
    }
   ],
   "source": [
    "## Finding the Optimal Top2Vec Model\n",
    "\n",
    "model_list = []\n",
    "num_top = []\n",
    "model_topics = []\n",
    "df[\"selftext\"] = df[\"selftext\"].values.astype('str')\n",
    "\n",
    "for min_count in range(5, 50, 5):\n",
    "    model = Top2Vec(list(df[\"selftext\"]), min_count, chunk_length=100, embedding_model=\"universal-sentence-encoder\")\n",
    "    model_topics.append(min_count)\n",
    "    model_list.append(model)\n",
    "    num_top.append(model.get_num_topics())\n",
    "    \n",
    "    ## Top2Vec Coherence Score, code from: https://github.com/Datanaught & https://github.com/ddangelov/Top2Vec/issues/158\n",
    "    import gensim.corpora as corpora\n",
    "    from gensim.utils import tokenize\n",
    "    from gensim.models import CoherenceModel\n",
    "\n",
    "    def t2vCoherence(df, topic_words):\n",
    "        tokenized = [list(tokenize(doc)) for doc in df.selftext.tolist()]\n",
    "        id2word = corpora.Dictionary(tokenized)\n",
    "        corpus = [id2word.doc2bow(text) for text in tokenized]\n",
    "    # make sure you grab the topic words from the topic model and convert them to a list\n",
    "        coherence_model = CoherenceModel(topics=topic_words, texts=tokenized, \n",
    "                                     corpus=corpus, dictionary=id2word, coherence='c_v', topn=50)  \n",
    "        coherence = coherence_model.get_coherence()\n",
    "        # print(\"Model Coherence C_V is:{0}\".format(coherence))\n",
    "        return coherence\n",
    "\n",
    "#    topic_words, word_scores, topic_nums = model.get_topics(model.get_num_topics())\n",
    "    topic_words, word_scores = model._find_topic_words_and_scores(model.topic_vectors) \n",
    "    \n",
    "    print(\"# Min Count: \" + str(min_count) + \" & Number of Topics: \" + str(model.get_num_topics()) + \" & Coherence:\" + str(t2vCoherence(df, topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e26ef-7139-428d-bb68-c791faeed3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
